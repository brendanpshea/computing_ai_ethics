# Computing and AI Ethics: Glossary of Key Concepts and Thinkers

## Chapter 1: The History of IT Ethics

| Term | Definition |
|------|-----------|
| **Writing** | External memory technology that stores information outside the human mind; enabled bureaucracy, law, and scholarship but also control. |
| **Socrates** | Ancient Athenian philosopher (c. 470–399 BCE) who distrusted writing and taught through dialogue and questioning. |
| **Plato** | Student of Socrates (c. 428–348 BCE) who founded the Academy in Athens; paradoxically, we know Socrates only through Plato's written dialogues. |
| **Socratic Method** | Technique of teaching through questioning and dialogue rather than direct instruction; associated with active learning and critical thinking. |
| **Phaedrus Dialogue** | Plato's work featuring the myth of Theuth and Thamus, wherein Socrates warns of writing's dangers to memory and true knowledge. |
| **Printing Press** | Revolutionary technology (~1440) that shifted from scarce manuscript copies to mass-produced books; enabled both enlightenment and propaganda. |
| **Johannes Gutenberg** | German inventor (c. 1400–1468) who developed movable type and printed the Gutenberg Bible; dramatically reduced book production cost. |
| **Martin Luther** | Reformer (1483–1546) who harnessed the printing press to spread written German Bible and 95 Theses across Europe; saw printing as providential. |
| **Erasmus of Rotterdam** | Renaissance scholar (1466–1536) who advocated gradual reform through scholarship and worried about unlearned masses misinterpreting complex texts. |
| **Telegraph** | Long-distance communication technology (~1840s) using electrical pulses and Morse code; enabled instant text transmission across continents. |
| **Morse Code** | System of dots and dashes representing letters and numbers; standard for telegraph communication. |
| **John Stuart Mill** | British philosopher and political economist (1806–1873) whose *On Liberty* examines threats to freedom from both government and social pressure. |
| **Karl Marx** | German philosopher and political economist (1818–1883) who analyzed how technology shapes economic systems and class relations. |
| **George Orwell** | British author (1903–1950) whose *1984* depicted dystopian mass surveillance enabled by communications technology. |
| **Hannah Arendt** | Political philosopher (1906–1975) who distinguished labor, work, and action; analyzed how technology shapes human condition. |
| **Alvin Toffler** | Futurist (1928–2016) who documented technological change and social disruption in *Future Shock*. |
| **Information Technology (IT)** | Technologies for creating, storing, transmitting, and processing information; an ancient concept including writing, printing, and digital systems. |
| **Normative Ethics** | Branch of philosophy concerned with how we *ought* to act; contrasts with descriptive ethics (what people believe) and metaethics (nature of morality itself). |
| **Five IT Revolutions** | Writing, Printing Press, Distance Communication, Mass Media, and the Web—each promised liberation while enabling new forms of control. |
| **Liberation-Control Cycle** | Pattern where each new IT technology promises liberation but paradoxically enables novel forms of control by those who command it. |

## Chapter 2: Virtue Ethics and Modern Technology

| Term | Definition |
|------|-----------|
| **Aristotle** | Ancient Greek philosopher (384–322 BCE); founder of systematic ethics emphasizing virtue, habit, and human flourishing. |
| **Virtue (Arête)** | Excellence in performing one's function; a stable character trait developed through practice and habituation. |
| **Eudaimonia** | Often translated as "flourishing" or "living well"; the highest human good realized through virtuous activity over a complete life. |
| **Doctrine of the Mean** | Aristotelian framework that virtue lies between extremes—courage between cowardice and recklessness; not numerical average but contextual balance. |
| **Phronesis** | Practical wisdom; the virtue of judgment that enables us to perceive what situations require and apply other virtues appropriately. |
| **Sophia** | Theoretical wisdom; understanding of universal principles and abstract knowledge. |
| **Episteme** | Scientific knowledge; systematic understanding of causes and principles. |
| **Nous** | Intuitive understanding or intellectual intuition; grasping first principles directly. |
| **Techne** | Craft or skill; practical knowledge of how to make things; applied art. |
| **Function Argument (Ergon)** | Aristotle's claim that ethics requires understanding humans' distinctive function—rationality—and excellence therein. |
| **Habituation** | Process of becoming virtuous through repeated practice; character is not innate but gradually formed through action. |
| **Lyceum** | School founded by Aristotle in Athens; emphasized empirical observation and systematic inquiry. |
| **Alasdair MacIntyre** | Contemporary philosopher (1929–) who revived virtue ethics; argues virtue requires social practices and narrative coherence. |
| **After Virtue** | MacIntyre's influential 1981 critique diagnosing modern moral fragmentation and loss of virtue traditions. |
| **Emotivism** | Metaethical view that moral claims express only personal preferences; lacks shared framework—MacIntyre's diagnosis of modernity. |
| **Narrative Unity** | Integration of life into a coherent story; MacIntyre argues we need narrative framework to understand what we ought to do. |
| **Rosalind Hursthouse** | Contemporary philosopher who defends virtue ethics as action-guiding through virtue-rules ("do what is honest"). |
| **V-Rules** | Virtue ethics action guidance focusing on character and motivation rather than prohibitions ("be generous" vs. "don't be stingy"). |
| **Martha Nussbaum** | Contemporary philosopher combining virtue ethics with capabilities approach; identifies central human capabilities necessary for flourishing. |
| **Capabilities Approach** | Framework identifying basic human capabilities (life, health, bodily integrity, practical reason, affiliation, play) necessary for dignity. |
| **Shannon Vallor** | Contemporary technophilosopher; extends virtue ethics to digital age through framework of "technomoral virtues." |
| **Technomoral Virtues** | Twelve virtues for 21st-century flourishing with technology: honesty, self-control, humility, justice, courage, empathy, care, civility, flexibility, perspective, magnanimity, and technomoral wisdom. |
| **Intellectual Virtues** | Character traits enabling good thinking: curiosity, intellectual humility, love of truth, open-mindedness, intellectual courage, autonomy. |
| **Moral Virtue** | Character trait enabling right action and feeling: courage, temperance, justice, generosity, honesty, friendliness. |
| **Nicholas Carr** | Author of *The Shallows*; argues internet use rewires brains for distraction and shallow processing instead of deep attention. |
| **Jonathan Haidt** | Moral psychologist who examines how technology hijacks attention and moral reasoning through variable reward schedules. |
| **Continuous Partial Attention** | State of divided focus and distraction from multi-tasking with multiple devices; undermines deep work. |
| **Cognitive Offloading** | Using technology as external memory rather than internal memory; remember *where* information is rather than *what* it is. |
| **Google Effect** | Research showing people remember *where* to find information (Google) rather than the information itself after offloading to search engines. |
| **Epistemic Bubble** | Situation where other voices are simply not heard; missing information; fixable through exposure. |
| **Echo Chamber** | Situation where other voices are actively discredited; distrust of outside sources; harder to escape than epistemic bubbles. |
| **Moral Deskilling** | Process where automation and delegation remove opportunities to develop moral and practical virtues through practice. |
| **William James** | American psychologist and philosopher (1842–1910) who emphasized voluntary attention as "root of judgment, character, and will." |

## Chapter 3: Freedom of Speech in the Digital Age

| Term | Definition |
|------|-----------|
| **John Stuart Mill** | British philosopher (1806–1873); primary theorist of freedom of speech in *On Liberty*. |
| **On Liberty** | Mill's 1859 foundational work examining limits of society's power over individuals; defends extensive speech protections. |
| **Liberalism** | Political tradition emphasizing individual liberty, rule of law, tolerance, and democratic governance; presumes freedom is default. |
| **Negative Freedom** | Freedom *from* interference; emphasis on removing barriers to action; shared across liberal tradition. |
| **Positive Freedom** | Freedom *to* achieve goals; requires resources, capabilities, and enabling conditions; contested between liberal traditions. |
| **Left-Liberalism** | Focuses on threats from corporate power and economic inequality; supports state regulation and safety nets. |
| **Libertarianism** | Focuses on threat from government; emphasizes minimal state and property rights; distrusts regulation. |
| **Classical Liberalism** | Historical position (Locke, Smith, Mill) progressive for its time; worried about corporate power alongside government. |
| **Harm Principle** | Mill's foundational principle: power can only be rightfully exercised to prevent harm to others; individual's own good is insufficient justification. |
| **Self-Regarding Actions** | In Mill's framework, actions affecting only oneself and not directly harming others; protected from restriction. |
| **Other-Regarding Actions** | In Mill's framework, actions affecting others and potentially causing harm; may be restricted. |
| **Fallibility Argument** | All silencing of discussion assumes infallibility; we have been wrong before and cannot know in advance which heretical ideas prove correct. |
| **Dead Dogma** | Unchallenged beliefs held without understanding; knowing the belief is true but not *why* it is true. |
| **Devil's Advocate** | Presenting contrary arguments to sharpen thinking; even wrong arguments strengthen understanding if one must defend against them. |
| **Marketplace of Ideas** | Metaphor comparing ideas to goods competing freely in markets; assumes truth has natural advantage in fair competition. |
| **Social Tyranny** | Suppression through social pressure, ostracism, and informal sanctions rather than formal law; "tyranny of prevailing opinion." |
| **Cancel Culture** | Modern manifestation of social pressure; public criticism and withdrawal of support in response to statements or actions. |
| **Deplatforming** | Removal of user or content from social media platform; exercise of corporate power over speech. |
| **Terms of Service** | Private rules governing platform use; resemble law but created and enforced by corporations rather than democratic process. |
| **First Amendment** | U.S. constitutional protection against government infringement on speech; no protection against private actors under traditional interpretation. |
| **Incitement** | Speech directed at producing imminent lawless action and likely to do so; can be restricted even by free-speech-protective systems. |
| **True Threats** | Serious statements intending to commit violence against identifiable targets; not protected speech. |
| **Defamation** | False statements of fact harming reputation; not protected speech. |
| **Fighting Words** | Direct verbal provocations likely to cause immediate violence; not protected speech. |
| **Obscenity Test** | U.S. standard for identifying unprotected obscene speech (prurient interest, patently offensive, lacking serious value); highly contested. |
| **Hate Speech** | Speech targeting groups based on identity attributes; most democracies restrict it; U.S. largely does not. |
| **Autonomy Argument** | Deontological case for free speech: restricts treat persons as incapable of judging ideas; violates human dignity and self-direction. |
| **Kant** | German philosopher (1724–1804) who argued persons must be treated as ends in themselves, never merely as means. |
| **Prior Restraint** | Government blocking speech before publication; considered especially harmful form of censorship. |
| **Sedition** | Speech inciting rebellion or opposition to government; historically restricted but increasingly protected. |
| **Blasphemy Laws** | Laws prohibiting defamation of religious figures or beliefs; common historically, rare in liberal democracies now. |

## Chapter 4: Intellectual Property in the Digital Age

| Term | Definition |
|------|-----------|
| **Intellectual Property (IP)** | Legal rights protecting creations of the mind; intangible, non-rivalrous, with artificial scarcity created by law. |
| **Copyright** | Legal right protecting expression in creative works (books, music, code); time-limited monopoly. |
| **Patents** | Legal right protecting novel, useful, non-obvious inventions; includes requirements for disclosure and time-limited monopoly. |
| **Trademarks** | Legal protection for source identifiers (names, logos); protects against consumer confusion; indefinite duration if renewed. |
| **Trade Secrets** | Confidential business information (formulas, algorithms, customer lists) protected by keeping secret; indefinite protection if secrecy maintained. |
| **Public Domain** | Creative works unprotected by copyright and available for free public use; includes expired copyrights and works never copyrighted. |
| **Fair Use** | Doctrine permitting limited copying without permission for criticism, education, parody, and transformative uses. |
| **Derivative Works** | Works based on or adapting existing works; copyright includes right to control derivative creations. |
| **Patent Bargain** | Inventor discloses how invention works (public learns) in exchange for 20-year monopoly on making, using, and selling. |
| **Novelty** | Requirement that patented invention must be new and not previously disclosed. |
| **Non-Obviousness** | Requirement that patent be non-obvious to someone skilled in the relevant field. |
| **Genericide** | Process where trademark becomes generic term (aspirin, escalator, zipper) and loses trademark protection. |
| **Thomas Jefferson** | U.S. founder and thinker; skeptical of intellectual property; compared ideas to light transmission. |
| **John Locke** | English philosopher (1632–1704); labor theory of property forms basis for some IP arguments. |
| **Statute of Anne** | First copyright law (1710); London: 14 years renewable once. |
| **Berne Convention** | 1886 international treaty harmonizing copyright protection across countries. |
| **TRIPS Agreement** | 1994 World Trade Organization agreement linking intellectual property to global trade. |
| **DMCA (Digital Millennium Copyright Act)** | 1998 U.S. law adding anti-circumvention provision (illegal to break digital locks) and safe harbors for platforms. |
| **DRM (Digital Rights Management)** | Technology that prevents copying and unauthorized use; enforced by DMCA anti-circumvention rules. |
| **Notice and Takedown** | DMCA provision allowing copyright holders to demand removal of content with notice to platform. |
| **Right to Repair** | Ability to repair owned devices; often blocked by copyright and DRM preventing access to repair information. |
| **Non-Rivalrous Goods** | Goods where use by one person does not prevent use by another (software, music); contrasts with physical property. |
| **Artificial Scarcity** | Scarcity created by law (copyright) rather than natural limitation; IP law creates artificial scarcity for information. |
| **Locke's Labor Theory** | Justification for property rights: people own fruits of their labor; used to justify IP. |
| **Lockean Proviso** | Locke's restriction: property rights valid only if "enough and as good" left for others. |
| **Utilitarian/Incentive Theory** | Justification for IP: necessary to incentivize creation by allowing recovery of investment; dominant legal justification. |
| **Personality/Expression Theory** | Justification for IP: creative works embody creator's personality; stronger in European tradition than U.S. |
| **Moral Rights (Droit Moral)** | Rights to attribution (credit) and integrity (no distortion); stronger in Europe than U.S. copyright system. |
| **James Boyle** | Contemporary scholar of intellectual property; coined phrase "enclosure of the intangible commons of the mind." |
| **Peter Drahos** | Scholar who frames contemporary IP expansion as "information feudalism" driven by powerful corporations. |
| **Information Feudalism** | Metaphor for IP system where corporations function as feudal lords controlling knowledge and imposing obligations. |
| **Cory Doctorow** | Contemporary technologist and writer; critiques IP overreach and advocates interoperability. |
| **Chokepoint Capitalism** | Doctorow's concept of digital bottlenecks that corporations control to extract value from creators. |
| **Tim Wu** | Scholar of attention economy and technology cycles; shows IP as tool for maintaining monopoly control. |
| **Napster** | Peer-to-peer file-sharing platform (1999–2001) that enabled music sharing; shut down by recording industry lawsuit. |
| **Streaming Model** | Distribution through access (Netflix, Spotify) rather than ownership; users rent access, platforms control content. |
| **Monopoly** | Exclusive right to make, use, and sell a patented invention during patent term. |
| **Monopsony** | Market situation where single powerful buyer exercises control; Doctorow argues platforms create monopsony control over creators. |
| **Disney** | Company engaged in extensive copyright extension lobbying; Sonny Bono Act sometimes called "Mickey Mouse Protection Act." |
| **Sonny Bono Act** | 1998 U.S. law extending copyright term by 20 years; retroactively lengthened protection for works already created. |
| **Retroactive Extension** | Extending copyright term on works already created cannot incentivize future creation; pure rent-seeking. |
| **Pharmaceutical Patents** | Patents on drugs that extend monopoly pricing capacity; raise ethical issues around access to medicine in poorer countries. |

## Chapter 5: The Political Philosophy of Cryptocurrency

| Term | Definition |
|------|-----------|
| **Cryptocurrency** | Digital currency using cryptography for security and operating on decentralized network; no central authority required. |
| **Bitcoin** | First and most well-known cryptocurrency; created January 3, 2009 by pseudonymous "Satoshi Nakamoto." |
| **Satoshi Nakamoto** | Pseudonymous creator of Bitcoin whose identity remains unknown; estimated holdings (~1.1M BTC) worth $100–135 billion at current prices, never moved. |
| **Blockchain** | Distributed public ledger recording transactions; immutable chain of blocks each containing cryptographic hash of previous block. |
| **Decentralization** | Absence of single point of control or failure; power distributed across network of nodes; foundational crypto principle. |
| **Peer-to-Peer (P2P)** | Direct transactions between parties without intermediary institution; contrasts with traditional banking model. |
| **Cryptography** | Mathematical techniques for securing information; cryptocurrency uses cryptographic signatures for authorization. |
| **Hash** | Cryptographic function converting input to fixed-size output; small input change produces completely different hash. |
| **Proof of Work** | Consensus mechanism requiring computational puzzle solving; used by Bitcoin; energy-intensive but highly secure. |
| **Proof of Stake** | Consensus mechanism requiring economic collateral rather than computation; used by Ethereum; more energy-efficient. |
| **Mining** | Process of discovering new blocks by solving computational puzzles (in Proof of Work systems); miners receive rewards. |
| **Block Reward** | New cryptocurrency created and awarded to miner who discovers valid block; gradually decreases (halving every ~4 years in Bitcoin). |
| **Halving** | Event reducing new Bitcoin block rewards by 50%; occurs approximately every 4 years; eventually all 21M Bitcoin mined by ~2140. |
| **Smart Contracts** | Self-executing code on blockchain enforcing agreements; enables complex transactions without intermediaries. |
| **Money** | Social technology functioning as medium of exchange, store of value, and unit of account; value based on collective belief. |
| **Fiat Currency** | Currency with value derived from government decree rather than intrinsic value or backing by commodity. |
| **Commodity Money** | Currency with intrinsic value (gold, shells); historically common before fiat systems. |
| **Representative Money** | Currency backed by commodity reserves (gold standard); 19th–20th century standard. |
| **Central Bank** | Government institution managing money supply and monetary policy; modern banking system built on trust in central banks. |
| **Fractional Reserve Banking** | Banking system where banks lend out most deposits (keep only fraction as reserve); enables credit creation but vulnerable to runs. |
| **Bank Run** | Mass withdrawal of deposits when confidence in bank fails; 2008 financial crisis demonstrated systemic risk. |
| **Sound Money** | Concept of currency resisting inflation and maintaining value; gold and Bitcoin positioned as "sound money" against fiat inflation. |
| **Inflation** | Increase in money supply reducing purchasing power; arguably hidden tax on savers and wage-earners. |
| **Deflation** | Decrease in prices and money supply; can discourage spending and investment, potentially harmful to economy. |
| **Monetary Inflation** | Growth in money supply (what Bitcoin maximalists object to); distinct from price inflation. |
| **Financial Privacy** | Ability to conduct transactions without surveillance; cash enabled this; cryptocurrency promises to restore it digitally. |
| **Financial Inclusion** | Access to banking services; ~1.3 billion adults globally lack bank access; cryptocurrency can provide access via internet. |
| **Friedrich Hayek** | Austrian economist (1899–1992); advocated "denationalization of money" and competition in currencies. |
| **Murray Rothbard** | Libertarian economist; advocated full commodity money and rejected central banking. |
| **Cypherpunk Movement** | 1990s movement of cryptographers and privacy advocates; slogan: "Privacy is necessary for an open society in the electronic age." |
| **Timothy May** | Cypherpunk founder; envisioned cryptographic technologies enabling anonymous economic transactions. |
| **Eric Hughes** | Cypherpunk co-founder; wrote "A Cypherpunk's Manifesto" (1993). |
| **Nick Szabo** | Cryptographer; developed bit gold (1998), precursor to Bitcoin. |
| **Hal Finney** | Early Bitcoiner and cryptographer; received first Bitcoin transaction; believed in Bitcoin's long-term potential. |
| **DigiCash** | Failed digital cash system (1989) using cryptography; predated Bitcoin. |
| **Double-Spending Problem** | Challenge in digital currency: preventing user from spending same digital unit twice; Bitcoin solved this without trusted authority. |
| **Consensus Mechanism** | Protocol enabling network to agree on valid transactions without central authority; Proof of Work and Proof of Stake are examples. |
| **Stablecoin** | Cryptocurrency designed to maintain stable value pegged to fiat currency or asset (e.g., Tether = $1 USD). |
| **DeFi (Decentralized Finance)** | Financial services (lending, trading, derivatives) built on blockchain without traditional intermediaries. |
| **NFT (Non-Fungible Token)** | Digital asset on blockchain proving unique ownership; controversial application of blockchain technology. |
| **DAO (Decentralized Autonomous Organization)** | Organization governed by smart contracts rather than traditional hierarchy; attempted governance without leadership. |
| **Altcoin** | Any cryptocurrency other than Bitcoin; Ethereum most significant but thousands exist. |
| **Ethereum** | Smart contract cryptocurrency platform (2015); enables applications beyond currency. |
| **El Salvador Case Study** | First country to adopt Bitcoin as legal tender (September 2021); revoked status as IMF loan condition (January 2025). |
| **Bitcoin Limited Supply** | Maximum 21 million Bitcoin will ever exist; algorithmically fixed scarcity; contrasts with fiat currency unlimited printing. |
| **Censorship Resistance** | Cryptocurrency property preventing government or corporation from blocking transactions; foundational to crypto advocates' philosophy. |
| **Decentralized Exchange (DEX)** | Financial market operating on blockchain without central authority; contrasts with traditional centralized exchanges. |

## Chapter 6: Privacy in the Information Age

| Term | Definition |
|------|-----------|
| **Privacy** | Difficult to define precisely; includes right to be let alone, control over personal information, and contextual integrity of information flows. |
| **Informational Privacy** | Control over personal data and information; prevents collection, use, and disclosure without consent. |
| **Physical Privacy** | Freedom from bodily intrusion and surveillance in intimate spaces. |
| **Decisional Privacy** | Autonomy over personal choices including reproduction and life direction. |
| **Communications Privacy** | Protection of correspondence and communications from interception. |
| **Associational Privacy** | Freedom to associate privately without disclosure of membership or affiliations. |
| **Intellectual Privacy** | Protection of private thoughts and beliefs; freedom from compelled speech. |
| **Contextual Integrity** | Helen Nissenbaum's theory: privacy is about appropriate information flows for particular contexts (medical info appropriate in hospital, not gossip). |
| **Warren & Brandeis** | Authors of seminal 1890 article "The Right to Privacy"; response to photographs and tabloid journalism. |
| **Fourth Amendment** | U.S. constitutional protection against unreasonable searches; foundational to privacy rights. |
| **Reasonable Expectation of Privacy** | Katz v. United States (1967) standard: person has privacy right where they reasonably expect privacy. |
| **Third-Party Doctrine** | Legal principle that information shared with third parties loses privacy protection; problematic in digital age. |
| **HIPAA** | Health Insurance Portability and Accountability Act; regulates health information privacy in U.S. |
| **GLBA (Gramm-Leach-Bliley Act)** | Regulates financial privacy and information security of financial institutions. |
| **COPPA** | Children's Online Privacy Protection Act; restricts data collection from children under 13. |
| **FERPA** | Family Educational Rights and Privacy Act; protects student education records. |
| **Sectoral Approach** | U.S. privacy regulation by sector (health, finance, education, etc.); contrast with comprehensive approach. |
| **GDPR (General Data Protection Regulation)** | Comprehensive EU privacy regulation (2018); regulates data collection, use, and rights; highest privacy protection globally. |
| **PIPL (Personal Information Protection Law)** | Chinese comprehensive privacy law (2021); prioritizes state control and national security over individual rights. |
| **CCPA/CPRA** | California's comprehensive privacy laws; gives consumers rights to access, delete, and opt out of data sales. |
| **Right to Erasure (Right to Be Forgotten)** | GDPR Article 17; right to request removal of personal information; Google Spain v. AEPD (2014) precedent. |
| **Data Portability** | GDPR right to obtain and reuse personal data across services; enables switching platforms. |
| **Consent Requirements** | GDPR principle requiring explicit, informed consent for data processing; "opt-in" rather than "opt-out" default. |
| **Judith Jarvis Thomson** | Philosopher arguing privacy is not distinct right but reducible to property and bodily rights. |
| **Richard Posner** | Economist arguing strong privacy is inefficient because it conceals information others have legitimate interests in knowing. |
| **Helen Nissenbaum** | Philosopher developing "contextual integrity" as framework for privacy; focuses on appropriateness of information flows. |
| **James Rachels** | Philosopher arguing privacy is constitutive of personhood and enables different relationships. |
| **Daniel Solove** | Privacy scholar emphasizing "aggregation problem": innocent data considered together can create damaging profiles. |
| **Nothing to Hide Argument** | Common excuse for accepting surveillance ("if you have nothing to hide, nothing to fear"); countered by aggregation and chilling effects. |
| **Chilling Effect** | Self-censorship and behavioral restriction from knowledge of surveillance; prevents expression and freedom. |
| **Privacy Paradox** | Observed gap between stated privacy concerns and actual privacy-protective behaviors; people say they value privacy but don't act accordingly. |
| **Bounded Rationality** | Cognitive limitations preventing fully rational decision-making; explains privacy paradox. |
| **Temporal Discounting** | Psychological tendency to favor immediate gratification over future consequences; privacy harms are abstract and distant. |
| **Data Broker** | Company collecting and selling personal data; ~5,000 brokers globally hold ~1,500+ databases on average American. |
| **Clearview AI** | Facial recognition company scraped 30+ billion photos from web without consent; case study in privacy violation. |
| **Algorithmic Surveillance** | Use of algorithms and AI for monitoring and collecting behavioral data at scale. |
| **Fair Information Practice Principles (FIPPs)** | 1973 foundational principles for data privacy: notice, choice, access, security, enforcement. |
| **FISA (Foreign Intelligence Surveillance Act)** | 1978 law creating secret court for surveillance warrants; attempted to regulate NSA surveillance. |
| **ECPA (Electronic Communications Privacy Act)** | 1986 law with problematic "180-day rule" giving less protection to older stored communications. |
| **USA PATRIOT Act** | Post-9/11 law expanding government surveillance; Section 215 enabled bulk phone metadata collection. |
| **National Security Letter** | Government demand for records without judicial oversight; used extensively post-9/11. |
| **Bulk Collection** | NSA mass collection of communications metadata; revealed by Snowden 2013. |
| **Edward Snowden** | NSA contractor who revealed classified surveillance programs (PRISM, Upstream, XKeyscore) in 2013. |
| **PRISM Program** | NSA program enabling direct access to tech company servers and data; revealed by Snowden. |
| **Upstream Collection** | NSA mass surveillance tapping fiber optic cables worldwide; revealed by Snowden. |
| **XKeyscore** | NSA surveillance tool capturing "nearly everything a user does on the Internet"; revealed by Snowden. |
| **USA FREEDOM Act** | 2015 law ending bulk phone metadata collection; partial response to Snowden revelations. |
| **Cambridge Analytica** | Political consulting firm that harvested 87 million Facebook profiles using "This Is Your Digital Life" app; used for political microtargeting. |
| **Psychographic Profiling** | Creating detailed psychological profiles of individuals based on data; used for targeted manipulation. |
| **Surveillance Capitalism** | Shoshana Zuboff's framework: extraction of human experience as data for profit; behavioral modification at scale. |
| **Behavioral Surplus** | Data beyond what's needed for service improvement; used to create prediction products for manipulation. |
| **Prediction Products** | Intelligence sold to business customers predicting and influencing future behavior. |
| **Shoshana Zuboff** | Scholar of surveillance capitalism; conceptualizes modern data extraction as systematic exploitation. |
| **Instrumentarian Power** | Zuboff's term for power exercised through shaping behavior at scale through algorithmic systems. |
| **Contact Tracing** | COVID-era surveillance technology tracking potential disease exposure; raised privacy concerns. |
| **Health Passports** | COVID-era digital records of vaccination or test status; raised privacy and freedom concerns. |
| **Facial Recognition** | Computer vision technology identifying people from images; used for surveillance; raises privacy and bias concerns. |
| **Biometric Identification** | Using biological characteristics (face, voice, fingerprint, DNA) for identification; increasingly common; raises privacy and security concerns. |
| **DNA Databases** | Genetic information collected by law enforcement and commercial services; raises privacy and discrimination concerns. |
| **Regime Type and Surveillance** | Surveillance scales with authoritarianism: totalitarian states use comprehensive surveillance; liberal democracies use targeted surveillance. |
| **Right to Exit** | Ability to leave a system; constrained by technology monopolies and lacks meaningful alternative platforms. |

## Chapter 7: Introduction to Artificial Intelligence

| Term | Definition |
|------|-----------|
| **Artificial Intelligence (AI)** | Umbrella term for computer systems performing tasks requiring human intelligence; definition contested and changes as systems improve. |
| **Alan Turing** | Computer scientist (1912–1954); father of computer science; proposed Turing Test as way to approach question of machine thought. |
| **Turing Test** | Imitation Game: test of machine intelligence by whether it can convince human interrogator it is human; behavior-based approach sidesteps metaphysical questions. |
| **Ada Lovelace** | First computer programmer (1815–1852); wrote notes on Babbage's Analytical Engine; argued machines cannot originate, only follow instructions. |
| **John McCarthy** | Computer scientist (1927–2011); coined "artificial intelligence" at 1956 Dartmouth Conference; developed LISP language. |
| **Marvin Minsky** | Co-founder MIT AI Lab (1927–2016); worked on early neural networks then symbolic AI; developed frames for knowledge representation. |
| **Tesler's Theorem** | "AI is whatever hasn't been done yet"; as AI systems succeed, we reclassify them as "not really AI." |
| **Moving Goalposts Problem** | Paradox that successful AI gets reclassified as mere computation; makes AI a moving target. |
| **Narrow vs. General AI** | Narrow: excels at single task (chess); General: performs any intellectual task; we currently have only narrow AI. |
| **Weak vs. Strong AI** | Weak: simulates intelligence; Strong: actually has consciousness/mind; philosophical divide remains unresolved. |
| **Symbolic AI (GOFAI)** | "Good Old-Fashioned AI"; approach using rules and logic; dominant 1950s–1980s; failed to achieve general intelligence. |
| **Connectionist AI** | Neural networks and learning-based approaches; contrasts with symbolic approaches; now dominant. |
| **Tool vs. Agent** | Tool: controlled by humans for tasks; Agent: acts autonomously toward goals; modern AI increasingly agent-like. |
| **Dartmouth Conference** | 1956 conference coining "artificial intelligence" and predicting rapid progress; optimism proved premature. |
| **Logic Theorist** | Early AI program (1956) proving mathematical theorems; claimed as first AI program. |
| **General Problem Solver** | Early attempt at domain-general reasoning (1959); influenced by means-ends analysis. |
| **SHRDLU** | Natural language understanding system in blocks world (1970); demonstrated language comprehension within constrained domain. |
| **AI Winter** | Period of reduced funding and interest due to unmet expectations; first winter 1974–1980, second 1987–1993. |
| **Expert Systems** | Knowledge-based systems encoding human expertise as rules (MYCIN, DENDRAL); failed due to knowledge bottleneck. |
| **Knowledge Bottleneck** | Challenge of extracting and encoding expertise; expert systems proved too labor-intensive to maintain. |
| **Machine Learning (ML)** | Instead of hand-coding rules, systems learn patterns from data; dominant AI paradigm. |
| **Supervised Learning** | Learning from labeled examples (data + correct answers); used for classification and regression. |
| **Unsupervised Learning** | Finding structure in unlabeled data; includes clustering and dimensionality reduction. |
| **Reinforcement Learning** | Learning from rewards and punishments; used in game playing, robotics, recommendation systems. |
| **Deep Learning** | Machine learning with multiple hidden layers; enables learning hierarchical representations. |
| **Neural Network** | System inspired by (but not identical to) biological neurons; weighted connections trained via backpropagation. |
| **Backpropagation** | Training algorithm for neural networks; efficiently computes gradients for weight updates. |
| **AlexNet** | Deep learning network (2012) dramatically reducing ImageNet error rate; sparked deep learning revolution. |
| **ImageNet** | Large image dataset used for computer vision benchmarking; ~14M labeled images. |
| **Computer Vision** | Systems recognizing images, faces, objects; major AI application area; raises privacy and bias concerns. |
| **Facial Recognition** | Computer vision task identifying people from images; increasingly used for surveillance; problematic for racial bias. |
| **Medical Imaging** | AI diagnosing diseases from medical images (tumors, diabetic retinopathy); raises accuracy and liability questions. |
| **Autonomous Vehicles (AVs)** | Self-driving cars that navigate without human control; decision pipeline: sense, perceive, predict, plan, act. |
| **Trolley Problem** | Philosophical dilemma: who should die in unavoidable accident; relevant to AV ethics. |
| **DARPA Grand Challenges** | 2004–2005 competitions for autonomous vehicles; spurred development of self-driving technology. |
| **Waymo** | Google's self-driving car company; operating robotaxi services. |
| **Cruise** | General Motors' self-driving car company; operating in San Francisco. |
| **Object Detection** | Identifying and locating objects in images; foundational to autonomous vehicles and robotics. |
| **Prediction Systems** | AI making consequential decisions about people: COMPAS (recidivism), credit scoring, hiring, healthcare, insurance. |
| **COMPAS** | Widely used recidivism prediction system; criticized for racial bias; case study in AI fairness. |
| **Recommendation Systems** | AI predicting what user wants (Netflix, YouTube, Amazon, TikTok); uses collaborative filtering, content-based, or hybrid approaches. |
| **Collaborative Filtering** | Recommendation approach: people like you liked X, so you might like it too. |
| **Content-Based Filtering** | Recommendation approach: you liked X, so you might like similar items. |
| **Algorithmic Recommendation** | Systems determining what content user sees; shapes information diet and can create filter bubbles. |
| **Filter Bubble** | Echo chamber effect from algorithmic recommendations showing mainly agreeable content. |
| **Natural Language Processing (NLP)** | AI understanding and generating human language; includes machine translation, question answering, text generation. |
| **Transformer Architecture** | Neural network architecture (2017) enabling efficient processing of sequences; foundation for large language models. |
| **Large Language Model (LLM)** | Neural networks trained on vast text; can generate human-like text and perform diverse language tasks. |
| **GPT (Generative Pre-trained Transformer)** | OpenAI's LLM series (GPT-3, GPT-4); demonstrates remarkable language abilities; raises questions about understanding vs. pattern matching. |
| **ChatGPT** | Conversational interface to GPT models; achieved 1M users in record time; sparked AI hype and concerns. |
| **Generative AI** | AI systems generating new content (text, images, code, music); distinguished from discriminative AI. |
| **John Searle** | Philosopher (1932–); Chinese Room argument: syntax (symbol manipulation) ≠ semantics (meaning); strong AI is false. |
| **Chinese Room Argument** | Searle's thought experiment: person following rules manipulating Chinese characters can pass Turing Test without understanding. |
| **Daniel Dennett** | Philosopher of mind; functionalist: if something functions like a mind, it *is* a mind; defends AI possibility. |
| **David Chalmers** | Philosopher; identifies "hard problem" of consciousness: why does mental processing feel like something? |
| **Stuart Russell** | AI researcher; *Artificial Intelligence: A Modern Approach* standard textbook; AI safety advocate. |
| **Luciano Floridi** | Philosopher of information; argues AI achieves "agency without intelligence"—effective action without understanding. |
| **Alignment Problem** | Challenge of ensuring AI systems behave as intended; central concern in AI safety. |
| **AI Safety** | Research field concerned with ensuring AI systems don't cause harm; includes alignment, robustness, transparency. |
| **Interpretability (Explainability)** | Making AI decisions understandable to humans; "black box" problem. |
| **Black Box Problem** | Some AI systems (deep learning) make decisions but their reasoning is opaque; problematic for accountability. |
| **Bias in AI** | Training data reflects historical discrimination; systems trained on biased data perpetuate discrimination. |
| **Racial Bias in Facial Recognition** | Facial recognition systems have higher error rates on darker skin tones; documented for major providers. |
| **Data Poisoning** | Intentionally corrupting training data to influence system behavior; security vulnerability. |
| **Adversarial Examples** | Slightly modified inputs fooling AI systems while imperceptible to humans; raises robustness concerns. |

## Chapter 8: AI and the Future of Human Labor

| Term | Definition |
|------|-----------|
| **Labor** | Cyclical survival activities leaving nothing behind; necessary but futile; consumed as produced (Arendt). |
| **Work** | Creating durable objects outlasting the maker; builds world of human artifice (Arendt). |
| **Action** | Disclosing oneself through deeds in public sphere; politics, speech, initiative (Arendt). |
| **Hannah Arendt** | Political philosopher (1906–1975); *The Human Condition* distinguishes labor, work, action; analyzes technology's role. |
| **Animal Laborans** | Arendt's term for human in state of labor; biological being focused on survival. |
| **Homo Faber** | Arendt's term for human as maker; creates durable objects; builds world. |
| **Meaningful Work** | Work providing autonomy, competence, relatedness, beneficence; more than just employment. |
| **Bullshit Jobs** | David Graeber's concept of employment that feels pointless and contributes nothing valued; damages morale. |
| **David Graeber** | Anthropologist; *Bullshit Jobs* argues many modern jobs lack purpose and social value. |
| **Autonomy** | Need to feel volitional and have choice; foundational to meaningful work. |
| **Competence** | Need to feel effective and capable; developing mastery over time. |
| **Relatedness** | Need for connection to others; valuable work involves collaboration and community. |
| **Beneficence** | Contributing to something beyond oneself; helping others; gives work purpose. |
| **Self-Determination Theory** | (Ryan & Deci) psychological theory: autonomy, competence, relatedness are psychological needs; satisfying them enables flourishing. |
| **Ryan & Deci** | Psychologists developing self-determination theory; research shows need satisfaction enables well-being and engagement. |
| **Martela & Ryan** | Researchers identifying four pathways to meaningful work: autonomy, competence, relatedness, beneficence. |
| **Identity and Self-Expression** | Work provides means of expressing self; "what do you do?" is primary identity question. |
| **Job Displacement** | Elimination of job category due to automation; raises economic and existential concerns. |
| **Deskilling** | Reduction of skill requirements in job; AI handles complex work, humans execute; undermines craft. |
| **Replacement Scenario** | AI eliminates jobs entirely; different sectors face different timelines and risks. |
| **Deskilling Scenario** | AI handles meaningful aspects; humans do routine execution; work becomes labor. |
| **Augmentation Scenario** | Optimistic scenario where AI handles labor, freeing humans for meaningful work. |
| **Luddites** | 1811 textile workers; smashed machines that deskilled craft; concerned with *quality* of work, not just employment. |
| **Charlie Chaplin** | Filmmaker; *Modern Times* (1936) depicts worker as literal machine cog; captures deskilling anxiety. |
| **McKinsey Report** | Estimates 400–800 million workers could be displaced globally by 2030. |
| **Truck Drivers** | 3.5 million U.S. truck drivers facing potential displacement from autonomous vehicles; middle-class income without college degree. |
| **Amazon Warehouse Workers** | Low-wage workers tracked by wristbands; algorithmically determined pace; constrained bathroom breaks; case study of deskilling. |
| **Algorithmic Management** | AI systems making decisions about worker pace, assignment, and behavior; undermines autonomy. |
| **Internal Goods** | (MacIntyre) excellences intrinsic to practice; good at arithmetic is external; understanding is internal. |
| **External Goods** | (MacIntyre) benefits extrinsic to practice; money, status, power; important but don't constitute practice. |
| **Practice** | (MacIntyre) coherent form of cooperative human activity with internal goods and standards of excellence. |
| **Eudaimonia** | Flourishing; highest human good; realized through virtuous activity in meaningful practices. |
| **Floridian Enveloping** | Luciano Floridi's concept that we adapt work environments to AI limitations; AI determines work structure. |
| **Radiologist Case Study** | Before AI: interprets images using expertise; after: reviews AI-flagged cases; diagnostic skill lost. |
| **Writer Case Study** | Before AI: crafts prose through revision; after: edits AI drafts; loses writerly voice and creative process. |
| **Designer Case Study** | Before AI: envisions and creates original designs; after: tweaks AI outputs; loses artistic vision. |
| **Programmer Case Study** | Before AI: architects solutions and solves problems; after: prompts and debugs AI code; loses architectural thinking. |
| **Chilling Effect on Intellectual Development** | If AI does thinking, humans don't develop thinking skills; analogous to calculator debate but deeper. |
| **Skill Atrophy** | Loss of abilities through disuse; if deskilled workers never practice, they cannot develop or maintain expertise. |
| **Utilitarian Ethics and Work** | Maximum preference satisfaction; meaningful work satisfies deep preferences but competes with consumer preferences. |
| **Deontological Ethics and Work** | Persons deserve treatment as ends; meaningful work treats as ends; deskilling treats as means. |
| **Virtue Ethics and Work** | Flourishing requires exercising virtues through practices; automation eliminates sites of virtue development. |
| **Dignity and Work** | Kantian respect for persons requires enabling meaningful work; deskilling violates dignity. |
| **Right to Meaningful Work** | Debate whether this is positive right (something owed) or negative right (freedom from interference). |
| **Job Guarantee** | Policy proposal to ensure employment for all at living wage; addresses job displacement directly. |
| **Universal Basic Income (UBI)** | Policy proposal for unconditional cash transfer to all citizens; alternative approach to job displacement. |
| **Techno-Utopianism** | Vision that technology solves human problems without moral costs; often dismisses concerns about deskilling. |
| **Techno-Pessimism** | Vision that technology undermines human flourishing; concerns about inevitable deskilling. |
| **Technology Bias** | Humans tend to overestimate technology benefits and underestimate costs; relevant to AI and work debates. |
| **Social Choice** | How we deploy technology reflects values; AI could augment or deskill depending on implementation. |
| **Creative Work** | Art, writing, design, research; Arendt's paradigm case of meaningful work; increasingly threatened by generative AI. |
| **Originality** | Capacity to create something new; questioned in age of AI trained on existing works. |
| **Voice** | Distinctive style and perspective; what generative AI cannot replicate but can simulate. |

